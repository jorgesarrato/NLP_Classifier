{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/jsarrato/PersonalProjects/ML_Portfolio/Sentiment_Analysis/Transformer/aclImdb_v1/aclImdb'\n",
    "\n",
    "test_data = {'text': [], 'label': []}\n",
    "train_data = {'text': [], 'label': []}\n",
    "\n",
    "data = {'train': train_data, 'test': test_data}\n",
    "\n",
    "for split in ['train', 'test']:\n",
    "    for label in ['pos', 'neg']:\n",
    "        path = f'{data_path}/{split}/{label}'\n",
    "        for file in os.listdir(path):\n",
    "            with open(f'{path}/{file}', 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                data[split]['text'].append(text)\n",
    "                data[split]['label'].append(1 if label == 'pos' else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Convert text to indices\n",
    "        text_indices = [self.vocab.get(word, self.vocab['<UNK>']) for word in text.split()[:self.max_len]] # Get token of word, or unknown. Cut excess after max_len\n",
    "        text_indices = text_indices + [self.vocab['<PAD>']] * (self.max_len - len(text_indices)) # Add padding\n",
    "        \n",
    "        return torch.tensor(text_indices, dtype=torch.long), torch.tensor(label, dtype=torch.float)\n",
    "\n",
    "def build_vocab(texts, max_vocab_size='auto'):\n",
    "    if max_vocab_size == 'auto':\n",
    "        max_vocab_size = len(set(word for text in texts for word in text.split())) + 2 # +2 for <PAD> and <UNK>\n",
    "\n",
    "    print(f\"Building vocabulary with max size: {max_vocab_size}\")\n",
    "\n",
    "    counter = Counter()\n",
    "    for text in texts:\n",
    "        counter.update(text.split())\n",
    "\n",
    "    vocab = {'<PAD>': 0, '<UNK>': 1} # Set token values for padding spaces or for unknown words\n",
    "    for word, _ in counter.most_common(max_vocab_size - 2):\n",
    "        vocab[word] = len(vocab)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary with max size: 262196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_texts, test_labels = data['test']['text'], data['test']['label']\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['train']['text'], data['train']['label'], test_size=0.1, random_state=SEED)\n",
    "\n",
    "vocab = build_vocab(train_texts)\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataset = IMDBDataset(train_texts, train_labels, vocab, MAX_LEN)\n",
    "val_dataset = IMDBDataset(val_texts, val_labels, vocab, MAX_LEN)\n",
    "test_dataset = IMDBDataset(test_texts, test_labels, vocab, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsarrato/BaseMLenv/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, hidden_dim, output_dim, dropout, max_len):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        # text: [batch_size, seq_len]\n",
    "        embedded = self.dropout(self.embedding(text) + self.positional_encoding[:, :text.size(1), :])\n",
    "        \n",
    "        # Transformer expects [seq_len, batch_size, embed_dim]\n",
    "        embedded = embedded.permute(1, 0, 2)\n",
    "        transformer_output = self.transformer(embedded, embedded)\n",
    "        \n",
    "        pooled = transformer_output.mean(dim=0)\n",
    "        \n",
    "        return self.fc(pooled).squeeze(1)\n",
    "\n",
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 2\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "model = TransformerModel(VOCAB_SIZE, EMBED_DIM, NUM_HEADS, NUM_LAYERS, HIDDEN_DIM, OUTPUT_DIM, DROPOUT, MAX_LEN).to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [03:26<1:05:30, 206.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n",
      "\tTrain Loss: 0.639 | Train Acc: 62.11%\n",
      "\tVal Loss: 0.620 | Val Acc: 74.69%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [06:52<1:01:49, 206.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "\tTrain Loss: 0.515 | Train Acc: 74.70%\n",
      "\tVal Loss: 0.524 | Val Acc: 78.05%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [10:18<58:26, 206.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n",
      "\tTrain Loss: 0.455 | Train Acc: 78.77%\n",
      "\tVal Loss: 0.560 | Val Acc: 79.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [13:47<55:12, 207.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n",
      "\tTrain Loss: 0.417 | Train Acc: 81.45%\n",
      "\tVal Loss: 0.563 | Val Acc: 79.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [17:16<51:55, 207.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5\n",
      "\tTrain Loss: 0.413 | Train Acc: 81.29%\n",
      "\tVal Loss: 0.533 | Val Acc: 79.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [20:45<48:35, 208.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6\n",
      "\tTrain Loss: 0.422 | Train Acc: 80.41%\n",
      "\tVal Loss: 0.529 | Val Acc: 79.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [24:11<44:59, 207.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7\n",
      "\tTrain Loss: 0.423 | Train Acc: 81.04%\n",
      "\tVal Loss: 0.781 | Val Acc: 72.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [27:34<41:14, 206.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8\n",
      "\tTrain Loss: 0.468 | Train Acc: 77.88%\n",
      "\tVal Loss: 0.682 | Val Acc: 73.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [31:04<38:00, 207.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9\n",
      "\tTrain Loss: 0.469 | Train Acc: 77.91%\n",
      "\tVal Loss: 0.735 | Val Acc: 73.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [34:27<34:18, 205.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "\tTrain Loss: 0.474 | Train Acc: 77.49%\n",
      "\tVal Loss: 0.561 | Val Acc: 77.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [37:50<30:46, 205.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "\tTrain Loss: 0.473 | Train Acc: 77.37%\n",
      "\tVal Loss: 0.802 | Val Acc: 73.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [41:17<27:24, 205.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "\tTrain Loss: 0.505 | Train Acc: 75.45%\n",
      "\tVal Loss: 0.745 | Val Acc: 72.19%\n"
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, labels = batch\n",
    "        text, labels = text.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text)\n",
    "        loss = criterion(predictions, labels)\n",
    "        acc = binary_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, labels = batch\n",
    "            text, labels = text.to(device), labels.to(device)\n",
    "            \n",
    "            predictions = model(text)\n",
    "            loss = criterion(predictions, labels)\n",
    "            acc = binary_accuracy(predictions, labels)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "N_EPOCHS = 20\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "import tqdm\n",
    "\n",
    "for epoch in tqdm.tqdm(range(N_EPOCHS)):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "    print(f\"\\tVal Loss: {val_loss:.3f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "# Test the model\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 22402:\n",
      "Text: Adored by fans for his unusually charming creativity and by Hollywood for his softball, user-friendly movie-making techniques, Tim Burton tipped the scales too far in formula's favor with his new upset of a cinematic legend, Sleepy Hollow. Following the quest of Ichabod Crane  played by Johnny Depp, delivering this dreary film's only shining point  to the heart of the mystery surrounding a town's seemingly random and gruesome murders by a fabled headless horseman, the story plays out as if it were purposely trying to be repugnantly predictable. Contrived as a children's bedtime story, humdrum character introduction is laced with intended-upon exciting non-engaging chase scenes which, with undeveloped characters fleeing for their lives, produce about as much fright and thrill as The Nightmare Before Christmas.<br /><br />Toss in an endless bundle of old trees for ambience and a wide-eyed, big-busted blonde love interest (Christina Reechi) and Burton has himself a movie that takes the age-old legend of Sleepy Hollow and succeeded in making it like a Disney movie without the charm or captivation. Dialog was choppy and ridiculous, severed heads were aplenty, and there were enough plot-revealing monologues to embarrass the likes of James Bond. Even with the backing of Emmanuel Lubezki, the most sought-after cinematographer in Hollywood today, the wonderful acting of Depp and Burton's astounding name-recognition, Sleepy Hollow is nothing to lose your head over.\n",
      "True Label: 0.0, Predicted: 0.95\n",
      "--------------------------------------------------\n",
      "Example 15957:\n",
      "Text: This odd little film starts out with the story of Bruno (Alex Linz) in a catholic school who has no friends and gets beat up everyday. He likes to wear dresses and his obese mother Angela who is a dressmaker doesn't think their is anything wrong with what her son likes. Angela complains to Mother Superior (Kathy Bates) but gets ignored and as the two of them walk back to they're car they are harassed by the other kids and are pelted with eggs. Bruno's father Dino (Gary Sinise) is divorced from Angela and is totally disgusted by his son being a sissy and practically disowns him. Bruno meets a new student at school named Shawniqua (Kiami Davael) who is a free spirit and dresses like Annie Oakley with cap pistols. Angela has a heart attack and Bruno's grandmother steps in to take care of him when Dino refuses.<br /><br />The film starts out with a very hard and unsympathetic look at all the characters involved. Angela has a great deal to do with Bruno wearing dresses as she practically encourages him. Dino was told when he was a young boy by his mother that he was a sissy because he liked opera and now he refuses to help Bruno when he needs it. The catholic school that Bruno attends is very unruly and all the kids run rampant and even call Shawniqua the \"N\" word. Once Shirley MacLaine steps in the film shifts and becomes more family oriented (So to speak). ****SPOILER ALERT**** The ending after the spelling bee is incredibly contrived and \"feel good\". Hugs and cheers for Bruno as reporters follow him and take his picture for their papers. All the while Shirley MacLaine is acting like the \"tough old broad\" who snaps at everyone. There is one thing about MacLaine's character in the film that no one has mention in these comments and it has to do with the masculine nature of her. I think the character of Helen might be a lesbian! She's very tough and strong and at one point in the film she shares a shot of whiskey with Bruno and smokes a cigar at the same time. I don't remember anyone in the film mentioning who her husband was or if she was ever married at all! This is why I think her character might be gay. Lots of other good actors appear in the film as well. Joey Lauren Adams, Jennifer Tilly, Brett Butler, Gwen Verdon and Lainie Kazan all should have taken a better look at the script before they signed on. I guess when they heard that MacLaine was directing that it would be an honor to be part of it. Very difficult to feel any remorse or understanding towards any of the characters and the subject matter is probably impossible for most to relate to. The actors are not bad but what exactly was MacLaine aiming for? Tolerance towards a young boy who wants to wear dresses and freedom of expression? We get that in the first 10 minutes, the rest of the time I was trying not to cringe.\n",
      "True Label: 0.0, Predicted: 0.99\n",
      "--------------------------------------------------\n",
      "Example 19072:\n",
      "Text: This movie was thoroughly unwholesome, unsettling and unsatisfying. Apart from a few nice shots of Italy, there's nothing to recommend this movie. As usual, Hollywood draws the wrong conclusion from a fractured existence--the _next_ guy you meet, whom you sleep with after knowing for a few hours, _he_ must be Mr. Right. As for humor, there is some in the movie, but I can't see how anyone could possibly label this a romantic _comedy_ since about three-quarters of the movie is totally depressing! My recommendation? Skip it in the theaters, wait till it comes out on DVD, then skip it there also. I want someone to give me back the two hours I wasted watching this dreck, drivel, dross.\n",
      "True Label: 0.0, Predicted: 0.58\n",
      "--------------------------------------------------\n",
      "Example 21857:\n",
      "Text: for everyone who has read this book, Fanny Price ends up maturing into her own woman, a beautiful woman...with a brain. Le Touzel looks like she is on medication. Terrible acting, she just ruins it! Henry is a little tall for his character. He is also too effeminate. Mary Crawford is brilliant. Edmond is a little too old. Mrs. Norris is hysterical- OK, this casting decision works. Rushworth is also perfect. Yates looks too effeminate also. But, Le Touzel is simply horrid. This is not a good character for her. Poor Fanny! I would recommend this movie only because it includes an almost complete textual account of the language Austen uses in the novel. The 1999 version is much more fun but terribly incomplete. If they could redo this version with a better suited actress for Fanny it would be fabulous!\n",
      "True Label: 0.0, Predicted: 0.92\n",
      "--------------------------------------------------\n",
      "Example 13246:\n",
      "Text: I got hold of a discount copy of this. I had seen it several years ago. My only recent experience had been \"Mystery Science Theatre\" where it was soundly spoofed. One never really gets a chance to get into these movies because of all the byplay. I love the beginnings of fifties horror movies. They give us a pompous lecture on the defense systems near the Arctic. These were there to protect us from the expected Soviet invasion, but they should come in handy, given the threat of very large insects. <br /><br />This particular one flies. For some reason, despite its exoskeleton made of the stuff grasshoppers are made of, they can still fend off air to air missiles and disable fighter planes. <br /><br />Anyway, it's more fun--first, the obligatory deranged case who saw the flying thing, cooling his heels in a hospital (it just teaches one--see an insect as big as a house--keep your mouth shut). I wonder if the poor guy got to go home after they found the bug.<br /><br />Otherwise, this is a pretty ordinary effort. It follows the usual efforts to come up with a way of dissuading the stubborn bug--and leaves us open to other possibilities--the Russians next time. I still get a kick out of these films and this one is serviceable.\n",
      "True Label: 0.0, Predicted: 1.00\n",
      "--------------------------------------------------\n",
      "Example 24676:\n",
      "Text: did anyone notice?when miss brook went skinny dipping,she left the water wearing white bikini bottoms and yet had previously taken it all off to join cabin boy.this could have been a good film without miss brooks phony accent and a year on the island please.how come that Kelly looked always clean and ready for a FM photo shoot.what started out with premise turned in to soft porn.and billy Zane come on,you cant be that hard up for film offers.check out dead calm.also when the people took her away ,how come she scoffed her face and after all that time didn't feel like throwing up.i suggest billy find decent scripts,Kelly stick to photo shoots and cabin boy play the son of Zorro in a future sequel.\n",
      "True Label: 0.0, Predicted: 0.98\n",
      "--------------------------------------------------\n",
      "Example 15261:\n",
      "Text: Spike Milligan was one of the funniest men I've ever seen, and a huge influence on my life.<br /><br />This movie is limp and awful, and does his memory no credit. The script is cluttered and preserves too many lines from the book intact (the leg jokes here are incomprehensible). The actors' performances are uniformly ineffective, a great cast wasted, and the lead, Sean Hughes, delivers Milligan's belligerent hostilities in a plaintive whine, which misses the point completely.<br /><br />The gentle pacing is a killer as well. Farce should accelerate towards the end. The Goon Shows often did, the novel \"Puckoon\" definitely did, but this film, if anything, slows down just when you want the various elements to smash together in a final climax.<br /><br />Milligan narrated an abridged audio recording of \"Puckoon\" in 1980, with T.P. McKenna, Dermot Kelly, Norma Ronald and Jack Hobbs. Now, that's funny. Ten minutes of that is funnier than this whole film. I believe the LP was transferred to CD, but don't know if it's still in print.<br /><br />There is a movie of \"Adolf Hitler: My Part in his Downfall\" with Jim Dale and Arthur Lowe. It too is a godawful mess, but it's funnier than this thing.<br /><br />It's possible that Milligan's spirit is too rambunctious for the screen. The other reviewers here are indulging in politeness and wishful thinking. This film fumbles virtually every opportunity and never misses a chance to disappoint.\n",
      "True Label: 0.0, Predicted: 0.99\n",
      "--------------------------------------------------\n",
      "Example 13570:\n",
      "Text: There was a time in the US that everything was possible on film, so came the roughies, movies containing horror and explicit scene's. The best known are Forced Entry and Waterpower, but of course those were made with a bit of budget. All shown on 42nd in NY, but hey, there were other grindhouses out there that showed no budget roughies. Wet Wilderness is an example of it. It circuited the underground scene after a while so copies were available but as seen on other reviews, some copies were abrupt cut at the end. But the version I watched was complete. Well i would call this one more a porn one then a roughie, there is a serial killer around but he likes more to watch others have sex instead of killing them, when he kills it's done off screen. The acting is the worst I ever seen. And I guessed that the so called actors didn't like what they are doing, for example in the beginning when we have the lesbian scene watch one girl stop performing and pulls a pubic hair out of here mouth then continues doing what was happening, or when mother is riding the black man, the daughter is sitting in the grass annoyed by ants! But it is the storyline that made this one famous, incest and racism is what this made it famous. When there is blood watch the two girls sitting there waiting for a cue to act, god this is worse but still one to have if you are into sleaze and grindhouse. Be sure that you have the full version.\n",
      "True Label: 0.0, Predicted: 0.96\n",
      "--------------------------------------------------\n",
      "Example 11562:\n",
      "Text: Mighty Like A Moose is one of many short films Director Leo McCarey did starring Charley Chase. What a dandy it is! Charlie and his wife both undergo plastic surgery to improve their hideous appearances unbeknown-est to each other. They then meet at a party and become smitten with each other. Now they can't allow each other to find out they're cheating. That's the preposterous premise of this frantic farce. Vivien Oakland, one of the few comic short leads to have a flourishing career long after the silents, is perfect as Charley's long of nose wife. Charley has an awful case of buck teeth, which are quickly dispatched at the dentist's. After a party is raided by police for no other reason then to practice raids, Charley and his wife frantically try avoiding each other at home for fear the alterations in appearances become known. Both have been photographed with their new features at the party. The hilarity back home culminates in Charley trying to teach the no-good-nick cheating with his wife a lesson. The no-good-nick of course is the new Charley, which his wife comes to realize long before Charley teaches a lesson in faithfulness. This is one of Charley Chase's better efforts. *** of 4 stars.\n",
      "True Label: 1.0, Predicted: 0.00\n",
      "--------------------------------------------------\n",
      "Example 11848:\n",
      "Text: In a small, picturesque Sicilian village, someone is brutally killing young, sexually curious boys. The local police force keep busy trying to track the killer and whittle the list down to five or so main suspects, including voyeuristic village retard Giuseppe (Vito Passeri) and an elusive, grungy, voodoo doll-poking backwoods witch named Maciara (Florinda Bolkan). There's also Don Alberto (Marc Popel), a handsome young priest who runs the local boy's school, Andrea (Tomas Milian), a journalist helping to aid the police, and the beautiful Patrizia (Barbara Bouchet), a gorgeous, but seriously screwed-up drug addict who seems to have a thing for very (I mean, VERY) young boys. As typical with the giallo subgenre, the plot won't be fully revealed until the last few frames, but if you can hang in there long enough, this film pays off. The script (written by Fulci, Gianfranco Clerici and Roberto Gianviti) keeps red herrings to a minimum and will keep you interested and the story is ably supported by excellent location work, cinematography (by Sergio D'Offizi) and musical score (by Riz Ortolani). The acting, particularly Bolkan, is also very good. Fulci fans who were weaned on his 80s grotesqueries like THE BEYOND and ZOMBIE will find more artistry and less gore on display here than they might anticipate, but they'll still enjoy a particularly nasty chain whipping scene in a cemetery (bizarrely, yet effectively, set to singer Ornella Vanoni's ballad \"Quei giorni insieme a te\") and a long tumble down a rocky embankment that Fulci liked so much that he reused it in his film THE PSYCHIC (1977). Scenes of the children being killed is mainly kept off screen (except for a brief strangulation), but the camera doesn't hesitate to linger on their corpses. The film was not released theatrically in America and, presumably because of some anti-Catholic elements in the storyline, received only a limited theatrical release in Europe.\n",
      "True Label: 1.0, Predicted: 0.00\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "misclassified = []\n",
    "\n",
    "for idx, test_data in enumerate(test_dataset):\n",
    "    text, label = test_data\n",
    "    text = text.unsqueeze(0).to(device)\n",
    "    prediction = model(text)\n",
    "    prediction = torch.sigmoid(prediction).item()\n",
    "    \n",
    "    if (prediction >= 0.5 and label == 0) or (prediction < 0.5 and label == 1):\n",
    "        misclassified.append((idx, test_texts[idx], label.item(), prediction))\n",
    "\n",
    "# Randomly select 10 misclassified examples to print\n",
    "random.shuffle(misclassified)\n",
    "for example in misclassified[:10]:\n",
    "    idx, text, true_label, predicted = example\n",
    "    print(f\"Example {idx}:\")\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"True Label: {true_label}, Predicted: {predicted:.2f}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BaseMLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
